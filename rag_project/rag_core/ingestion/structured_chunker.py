from __future__ import annotations

import logging
import re
from dataclasses import dataclass
from enum import IntEnum
from typing import Callable, List, Optional, Sequence

from rag_project.config import (
    BOUNDARY_PATTERN_WEIGHTS,
    BOUNDARY_PRIORITY_VALUES,
    BOUNDARY_BLANKLINE_BOOST,
    BOUNDARY_CONFIDENCE_THRESHOLD,
    CHUNK_ASSIST_MAX_OUTPUT_TOKENS,
    CHUNK_BOUNDARY_PATTERNS,
    MIN_SPLIT_RATIO,
    NOISE_ARTIFACT_REGEX,
    NOISE_CAPTION_TOKENS,
    NOISE_DOTTED_TOC_REGEX,
    NOISE_PAGE_REGEX,
    NOISE_REFERENCE_REGEX,
    NOISE_REFERENCE_TOKENS,
    NOISE_TABLE_REGEX,
    SCRUB_CAPTION_REGEX,
    SCRUB_PAGE_REGEX,
    SCRUB_TABLE_REGEX,
    SEGMENT_MAX_WORDS_MESSAGE,
    SPLIT_WEIGHT_FORMULA,
    STRUCTURED_BOUNDARY_PROMPT,
    STRUCTURED_DEFAULT_PROXIMITY_WEIGHT,
    STRUCTURED_MAX_CHUNK_WORDS_HARD,
    STRUCTURED_MAX_LLM_INPUT_WORDS,
    STRUCTURED_MIN_CHUNK_WORDS,
    STRUCTURED_OVERSIZE_FACTOR,
    PRIORITY_SPLIT_PATTERNS,
    BOUNDARY_INCLUSIVE_OFFSET,
)
from rag_project.rag_core.ingestion.chunker import chunk_text
from rag_project.logger import get_logger


@dataclass
class ChunkConfig:
    max_chunk_words: int
    overlap_words: int
    min_chunk_words: int = STRUCTURED_MIN_CHUNK_WORDS
    max_chunk_words_hard: int = STRUCTURED_MAX_CHUNK_WORDS_HARD
    use_llm: bool = False
    max_llm_input_words: int = STRUCTURED_MAX_LLM_INPUT_WORDS
    proximity_weight: float = STRUCTURED_DEFAULT_PROXIMITY_WEIGHT


@dataclass
class Boundary:
    index: int  # line index where a boundary occurs (inclusive end of chunk)
    confidence: float


class BoundaryPriority(IntEnum):
    """Lower number = higher priority (better split point)."""

    SECTION_HEADER, DATE_ROLE_BLOCK, PARAGRAPH_BREAK, SENTENCE_END, BULLET_END, FALLBACK = BOUNDARY_PRIORITY_VALUES


@dataclass
class BoundaryCandidate:
    position: int
    priority: BoundaryPriority
    distance_to_target: int

    def score(self) -> tuple[int, int]:
        return (self.priority, self.distance_to_target)


BOUNDARY_SIGNALS: dict[str, tuple[re.Pattern, float]] = {
    name: (re.compile(CHUNK_BOUNDARY_PATTERNS[name]), BOUNDARY_PATTERN_WEIGHTS.get(name, 0.0))
    for name in CHUNK_BOUNDARY_PATTERNS
}

logger = get_logger(__name__)

# --- NOISE FILTERING & CLEANING LOGIC ---

def _is_code_line(line: str) -> bool:
    """Returns True if the line looks like Python/SQL/C++ code to protect it from filtering."""
    l = line.strip()
    
    # 1. Common Keywords (Python/General)
    if l.startswith(("def ", "class ", "import ", "from ", "return ", "print(", "logger.", "@")):
        return True
        
    # 2. Indentation + Keywords (e.g. "    if x:", "  return")
    if line.startswith((" ", "\t")):
        if l.startswith(("if ", "for ", "while ", "try:", "except:", "else:", "elif ", "with ")):
            return True
            
    # 3. Variable assignments (e.g. "x = 5", "df = pd.read_csv")
    if re.match(r"^[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*[^=]", l):
        return True
        
    return False

def _scrub_raw_text(text: str) -> str:
    """
    Aggressively removes tables and captions from the raw text block 
    BEFORE splitting into lines.
    """
    # 1. Remove Markdown Tables (Lines starting with |)
    # Regex: Start of line, optional whitespace, pipe, anything, end of line
    text = re.sub(SCRUB_TABLE_REGEX, '', text, flags=re.MULTILINE)
    
    # 2. Remove Figure/Table Captions (even if inline)
    # Matches "**Figure 8.7:**" or "**Table 1:**" and the rest of the line
    text = re.sub(SCRUB_CAPTION_REGEX, '', text, flags=re.IGNORECASE)
    
    # 3. Remove specific academic noise artifacts
    text = re.sub(SCRUB_PAGE_REGEX, '', text, flags=re.IGNORECASE)
    
    return text

def _is_noise_line(line: str) -> bool:
    """Detects likely PDF artifacts, page numbers, or caption noise."""
    
    # --- SAFETY CHECK: PROTECT CODE ---
    # We must check this first to avoid deleting code that prints "Figure..."
    if _is_code_line(line):
        return False
    # ----------------------------------

    l = line.strip().lower()
    
    # 1. Empty lines
    if not l:
        return False

    # 2. Page numbers (Aggressive)
    # Matches "Page 85", "85", "Page 10 of 20"
    if re.match(NOISE_PAGE_REGEX, l):
        return True
        
    # 3. Table of Contents Dotted Lines (Visual Noise)
    # Matches lines that are mostly dots: "Introduction ........... 5"
    if re.search(NOISE_DOTTED_TOC_REGEX, l):
        return True

    # 4. Markdown/ASCII Tables
    # Matches "|---|---|" or lines starting with pipe
    if l.startswith("|") or re.match(NOISE_TABLE_REGEX, l):
        return True

    # 5. Artifact Headers/Footers
    # Matches "_Chapter Name_" often generated by pymupdf4llm
    if re.match(NOISE_ARTIFACT_REGEX, l): 
        return True

    # --- CONTENT ANALYSIS (Clean up markdown syntax) ---
    # Remove *, #, and _ (Italics/Bold often wrap references)
    clean_text = l.replace('*', '').replace('#', '').replace('_', '').strip()
    
    # 6. Explicit Blocklist (Captions)
    # Check if line STARTS with these (e.g. "Abbildung 8.1:")
    if clean_text.startswith(NOISE_CAPTION_TOKENS):
        return True
    
    # 7. References, Bibliography & TOC Headers
    # Exact match (e.g. "References")
    if clean_text in NOISE_REFERENCE_TOKENS:
        return True
    
    # Matches numbered headers: "10. Attachment" or "A. List of Figures"
    if re.match(NOISE_REFERENCE_REGEX, clean_text):
        return True
        
    return False

def _clean_segment_text(text: str) -> str:
    """Pipeline to clean text before chunking."""
    # Step 1: Scrub patterns (Tables, Inline Captions) using Regex
    text = _scrub_raw_text(text)
    
    # Step 2: Filter specific lines (Page numbers, Headers)
    lines = text.splitlines()
    cleaned = [line for line in lines if not _is_noise_line(line)]
    
    return "\n".join(cleaned)

# ------------------------------------

def _best_signal_score(line: str) -> float:
    scores = [score for pattern, score in BOUNDARY_SIGNALS.values() if pattern.search(line)]
    return max(scores) if scores else 0.0


def _dedup_lines(text: str) -> str:
    """Remove exact consecutive duplicates and overlapping prefix/suffix patterns."""
    def find_overlap(line1: str, line2: str) -> str:
        max_overlap = min(len(line1), len(line2))
        for length in range(max_overlap, 0, -1):
            if line1[-length:] == line2[:length]:
                return line1[-length:]
        return ""

    lines = text.split("\n")
    cleaned: List[str] = []
    i = 0
    while i < len(lines):
        current = lines[i].strip()
        if not current:
            if not cleaned or cleaned[-1] != "":
                cleaned.append("")
            i += 1
            continue

        if i + 1 < len(lines):
            next_line = lines[i + 1].strip()
            # Case: next starts with current, trim prefix from next
            if next_line.startswith(current) and next_line != current:
                cleaned.append(current)
                lines[i + 1] = next_line[len(current):].strip()
                i += 1
                continue
            # Case: current contains next (skip next if it's short)
            if next_line and next_line in current and len(next_line) > 5:
                cleaned.append(current)
                i += 2
                continue
            # Case: overlapping suffix/prefix
            overlap = find_overlap(current, next_line)
            if overlap and len(overlap) > 10:
                cleaned.append(current[:-len(overlap)].strip())
                i += 1
                continue

        if not cleaned or current != cleaned[-1]:
            cleaned.append(current)
        i += 1

    return "\n".join([line for line in cleaned if line is not None]).strip()


def detect_boundaries(lines: Sequence[str]) -> List[Boundary]:
    """Return candidate boundaries with confidence scores based on line patterns."""
    boundaries: List[Boundary] = []
    total = len(lines)
    for idx, line in enumerate(lines):
        score = _best_signal_score(line)
        # Boost if a blank line precedes an obvious header or date line.
        if line.strip() == "" and idx + 1 < total:
            next_score = _best_signal_score(lines[idx + 1])
            score = max(score, min(1.0, next_score + BOUNDARY_BLANKLINE_BOOST))
        if score >= BOUNDARY_CONFIDENCE_THRESHOLD:
            boundaries.append(Boundary(index=idx, confidence=score))
    logger.debug("Detected %s boundaries", len(boundaries))
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug("Boundary indices: %s", [b.index for b in boundaries][:50])
    return boundaries


def _word_positions(text: str) -> List[int]:
    return [m.start() for m in re.finditer(r"\S+", text)]


def _find_all_priority_boundaries(text: str, min_pos: int, target_pos: int) -> List[BoundaryCandidate]:
    candidates: List[BoundaryCandidate] = []
    patterns = {
        BoundaryPriority.SECTION_HEADER: PRIORITY_SPLIT_PATTERNS["section_header"],
        BoundaryPriority.DATE_ROLE_BLOCK: PRIORITY_SPLIT_PATTERNS["date_role_block"],
        BoundaryPriority.PARAGRAPH_BREAK: PRIORITY_SPLIT_PATTERNS["paragraph_break"],
        BoundaryPriority.SENTENCE_END: PRIORITY_SPLIT_PATTERNS["sentence_end"],
        BoundaryPriority.BULLET_END: PRIORITY_SPLIT_PATTERNS["bullet_end"],
    }
    for priority, pattern in patterns.items():
        for match in re.finditer(pattern, text):
            pos = match.end() if priority == BoundaryPriority.PARAGRAPH_BREAK else match.start()
            if min_pos <= pos <= target_pos:
                candidates.append(
                    BoundaryCandidate(
                        position=pos,
                        priority=priority,
                        distance_to_target=abs(target_pos - pos),
                    )
                )
    return candidates


def _find_safe_split_point(text: str, target_pos: int, min_pos: int, proximity_weight: float) -> int:
    candidates = _find_all_priority_boundaries(text, min_pos, target_pos)
    if not candidates:
        return target_pos
    max_distance = max(1, target_pos - min_pos)

    def weighted_score(c: BoundaryCandidate) -> float:
        # Normalize priority (1..5) to 0..1
        priority_score = (c.priority - 1) / 4
        distance_score = c.distance_to_target / max_distance
        return (1 - proximity_weight) * priority_score + proximity_weight * distance_score

    candidates.sort(key=weighted_score)
    best = candidates[0]
    logger.debug(
        "Split candidates=%s best_pos=%s priority=%s distance=%s",
        len(candidates),
        best.position,
        best.priority,
        best.distance_to_target,
    )
    return candidates[0].position


def _split_by_boundaries(lines: Sequence[str], boundaries: Sequence[Boundary]) -> List[str]:
    if not boundaries:
        return ["\n".join(lines).strip()]
    sorted_bounds = sorted(boundaries, key=lambda b: b.index)
    chunks: List[str] = []
    start = 0
    for b in sorted_bounds:
        end = b.index + BOUNDARY_INCLUSIVE_OFFSET  # include boundary line
        if end > start:
            chunks.append("\n".join(lines[start:end]).strip())
            start = end
    if start < len(lines):
        chunks.append("\n".join(lines[start:]).strip())
    return [c for c in chunks if c]


def _apply_overlap(chunks: List[str], overlap_words: int) -> List[str]:
    if overlap_words <= 0 or len(chunks) <= 1:
        return chunks
    overlapped: List[str] = []
    prev_tail: List[str] = []
    for idx, chunk in enumerate(chunks):
        words = chunk.split()
        if idx > 0 and prev_tail:
            combined = prev_tail + words
            overlapped.append(" ".join(combined))
        else:
            overlapped.append(chunk)
        prev_tail = words[-overlap_words:] if len(words) > overlap_words else words
    return overlapped


def _enforce_max_size(chunks: List[str], max_words: int, overlap_words: int) -> List[str]:
    """Further split oversized chunks using the existing sentence-aware chunker."""
    final_chunks: List[str] = []
    for chunk in chunks:
        words = chunk.split()
        if len(words) > max_words * STRUCTURED_OVERSIZE_FACTOR:
            final_chunks.extend(chunk_text(chunk, max_tokens=max_words, overlap_tokens=overlap_words))
        else:
            final_chunks.append(chunk)
    return final_chunks


def _llm_boundaries(
    lines: Sequence[str],
    hint_boundaries: Sequence[Boundary],
    llm_generate: Callable[[str, int], str],
) -> List[Boundary]:
    """Ask a lightweight LLM to suggest boundary line numbers; validate and return."""
    text = "\n".join(lines)
    hint_lines = [b.index for b in hint_boundaries]
    prompt = STRUCTURED_BOUNDARY_PROMPT.format(lines=text, hints=hint_lines)
    try:
        raw = llm_generate(prompt, CHUNK_ASSIST_MAX_OUTPUT_TOKENS)
    except Exception:
        return []
    try:
        import json

        data = json.loads(raw)
        raw_bounds = data.get("boundaries", [])
        validated: List[Boundary] = []
        for val in raw_bounds:
            if isinstance(val, int) and 0 <= val < len(lines):
                validated.append(Boundary(index=val, confidence=0.9))
        validated = sorted({b.index: b for b in validated}.values(), key=lambda b: b.index)
        return validated
    except Exception:
        return []


def _segment_large_text(text: str, limit_words: int) -> List[str]:
    """Heuristically split very large text into segments below the LLM input cap."""
    words = text.split()
    if len(words) <= limit_words:
        return [text]
    logger.info(SEGMENT_MAX_WORDS_MESSAGE, len(words), limit_words)
    segments: List[str] = []
    current: List[str] = []
    count = 0
    paragraphs = text.split("\n\n")
    for para in paragraphs:
        para_words = para.split()
        if count + len(para_words) > limit_words and current:
            segments.append("\n\n".join(current).strip())
            current = [para]
            count = len(para_words)
        else:
            current.append(para)
            count += len(para_words)
    if current:
        segments.append("\n\n".join(current).strip())
    return [s for s in segments if s]


def _split_to_size(chunks: List[str], config: ChunkConfig) -> List[str]:
    """Ensure chunks respect target size using priority-based splits."""
    sized: List[str] = []
    for chunk in chunks:
        remaining = chunk
        while True:
            words = remaining.split()
            if len(words) <= config.max_chunk_words:
                sized.append(remaining.strip())
                break
            positions = _word_positions(remaining)
            if not positions:
                sized.append(remaining.strip())
                break
            target_idx = min(len(positions) - 1, config.max_chunk_words)
            target_pos = positions[target_idx]
            min_idx = min(len(positions) - 1, max(config.min_chunk_words, int(config.max_chunk_words * MIN_SPLIT_RATIO)))
            min_pos = positions[min_idx]
            split_pos = _find_safe_split_point(remaining, target_pos, min_pos, config.proximity_weight)
            # guard against bad splits
            if split_pos <= 0 or split_pos >= len(remaining) - 1:
                # fallback to sentence-aware split
                fallback_chunks = chunk_text(remaining, max_tokens=config.max_chunk_words, overlap_tokens=0)
                sized.extend([c.strip() for c in fallback_chunks if c.strip()])
                break
            sized.append(remaining[:split_pos].strip())
            remaining = remaining[split_pos:].lstrip()
            if not remaining:
                break
    # If nothing split (still oversized), force fallback split
    forced: List[str] = []
    for item in sized:
        if len(item.split()) > config.max_chunk_words_hard:
            logger.warning(
                "Forcing split of oversized chunk (%s words > %s)",
                len(item.split()),
                config.max_chunk_words_hard,
            )
            forced.extend(chunk_text(item, max_tokens=config.max_chunk_words, overlap_tokens=0))
        else:
            forced.append(item)
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(
            "Split to size: target=%s chunks=%s sizes=%s",
            config.max_chunk_words,
            len(forced),
            [len(c.split()) for c in forced][:20],
        )
    return forced


def chunk_structured(
    text: str,
    config: ChunkConfig,
    llm_generate: Optional[Callable[[str, int], str]] = None,
) -> List[str]:
    """
    Structure-aware chunking:
    - Clean text (remove noise like figures/tables/page numbers)
    - detect boundaries via pattern signals
    - split on boundaries
    - enforce max size with sentence-aware fallback
    - apply overlap between chunks
    - optionally refine boundaries with a lightweight LLM when enabled and below size limits
    """
    
    # --- STEP 1: CLEANING PASS ---
    text = _clean_segment_text(text)
    # -----------------------------

    segments = _segment_large_text(text, config.max_llm_input_words if config.use_llm else config.max_chunk_words * 3)
    all_chunks: List[str] = []
    for segment in segments:
        lines = _dedup_lines(segment).splitlines()
        boundaries = detect_boundaries(lines)
        if config.use_llm and llm_generate and len(segment.split()) <= config.max_llm_input_words:
            llm_bounds = _llm_boundaries(lines, boundaries, llm_generate)
            if llm_bounds:
                boundaries = llm_bounds
                logger.info("Applied LLM boundaries: %s", [b.index for b in boundaries])
        initial_chunks = _split_by_boundaries(lines, boundaries)
        sized_chunks = _split_to_size(initial_chunks, config)
        overlapped = _apply_overlap(sized_chunks, config.overlap_words)
        # Merge too-short tails with previous chunk to avoid tiny fragments.
        merged: List[str] = []
        for chunk in overlapped:
            if merged and len(chunk.split()) < config.min_chunk_words and len(merged[-1].split()) < config.min_chunk_words:
                merged[-1] = merged[-1] + "\n\n" + chunk
            else:
                merged.append(chunk)
        merged = [_dedup_lines(c) for c in merged]
        all_chunks.extend(merged)
        logger.info("Segment produced %s chunks (sizes=%s)", len(merged), [len(c.split()) for c in merged])
    return all_chunks
