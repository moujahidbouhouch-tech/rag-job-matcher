{
  "jobs": [
    {
      "title": "Senior RAG Engineer (DummyCity1)",
      "company": "LambdaDummy AI",
      "location": "DummyCity1, Germany",
      "language": "en",
      "url": "https://example.com/jobs/rag-engineer-dummycity1",
      "description": "We are looking for a Senior Retrieval-Augmented Generation (RAG) Engineer to design, build, and harden our customer-facing question-answering stack. You will own the full RAG lifecycle: ingestion pipelines, normalization, chunking, embeddings, vector search, and prompt orchestration. You will work with multilingual corpora (German/English) and enforce strong observability so that every answer is traceable back to citations and versioned documents. The role is hands-on: you will prototype with Python, FastAPI, PostgreSQL/pgvector, and Ollama-hosted LLMs, and you will ship those prototypes into production with Docker and CI/CD. You will collaborate closely with product managers and data scientists to translate business intents into robust retrieval queries and evaluation suites. \n\nKey responsibilities include building resilient ingestion flows that handle PDFs, Word, and HTML, normalizing messy enterprise text, and chunking documents in a structure-aware fashion so that semantic boundaries are respected. You will design embeddings evaluations, tune similarity metrics, and implement guardrails for prompt templates. You will own latency and cost budgets: optimizing batch embedding jobs, controlling context window usage, and preventing runaway token costs. You will integrate synthetic data generation for recall testing, build offline regression tests, and establish dashboards to track coverage, hallucination rate, and response time. \n\nThe ideal candidate has shipped at least one RAG system to production and has battle scars from dealing with edge cases like duplicated lines, malformed PDFs, and inconsistent metadata. You are comfortable profiling bottlenecks in Python and SQL, instrumenting services with structured logging, and adding circuit breakers around LLM calls. You have opinions on chunking strategies (fixed-size vs structure-aware), overlap sizing, and how to choose embedding models based on domain language and latency requirements. You can explain trade-offs between approximate nearest neighbor indexes and brute-force search, and you know how to validate similarity scoring with human and synthetic benchmarks. \n\nBonus points for experience with GPU scheduling, quantization, or low-rank adapters, as well as exposure to European data privacy requirements. We value engineers who keep things simple: clear interfaces, readable configs, and minimal knobs for operators. If you want to build a reliable RAG platform that users actually trust, and you enjoy collaborating with a small team that ships fast, this role is for you."
    },
    {
      "title": "Applied LLM Engineer â€“ AI Assistant Platform (DummyCity2)",
      "company": "DeepStack Dummy",
      "location": "DummyCity2, Germany",
      "language": "en",
      "url": "https://example.com/jobs/applied-llm-dummycity2",
      "description": "Join our Applied LLM team to extend our conversational assistant that supports internal analysts and customer success teams. You will design prompts, retrieval strategies, and evaluation loops that keep answers grounded in our knowledge base. Your day-to-day will range from writing Python services that call Ollama-hosted models, to tuning chunk sizes and overlap for technical manuals, to adding health checks and warm-up routines so latency stays predictable. You will partner with product to define success metrics (groundedness, coverage, deflection) and build offline tests that capture tricky user intents like multilingual queries, mixed structured and unstructured inputs, and requests that require time filtering. \n\nYou will own the retrieval layer: ensuring that metadata filters (language, region, product, version) are always applied, handling recency with decay functions, and preventing low-quality chunks from polluting results. You will implement structured chunking that favors semantic boundaries over naive fixed windowing, and you will experiment with lightweight LLMs (e.g., qwen2.5 1.5B) for boundary refinement when documents are noisy. Observability matters: you will emit structured logs for chunking decisions, embedding calls, similarity scores, and prompt invocations so that downstream debugging is straightforward. \n\nThe stack includes FastAPI, Typer CLIs for ops, PostgreSQL with pgvector, sentence-transformers for embeddings, and a small front-end for internal users. You will containerize services, set up CI checks (lint, tests, formatting), and work with DevOps to roll out blue/green deployments. If you enjoy combining deep LLM intuition with pragmatic engineering, and you like writing clear documentation for everything you ship, you will thrive in this role. Expect to spend time triaging messy source documents, adding guardrails against hallucinations, and collaborating with stakeholders who want trustworthy, cited answers rather than flashy demos. \n\nWe welcome candidates who can communicate clearly, challenge assumptions, and propose simple architectures that are easy to maintain. Prior experience in Germany and comfort working across English/German content is helpful, but not required."
    },
    {
      "title": "Search & Data Platform Engineer (DummyCity3)",
      "company": "VectorForge Dummy",
      "location": "DummyCity3, Germany",
      "language": "en",
      "url": "https://example.com/jobs/search-platform-dummycity3",
      "description": "As a Search & Data Platform Engineer, you will modernize our document search and discovery capabilities for a portfolio of industrial clients. The mission is to replace brittle keyword search with semantic retrieval augmented by domain-tuned embeddings and clear, auditable answers. You will design ingestion flows that parse heterogeneous sources: PDFs with two-column layouts, Markdown docs, HTML help centers, and CSV exports. You will normalize and de-duplicate content, chunk it with overlap tuned to preserve section coherence, and store it in a Postgres + pgvector stack. On top of that, you will expose APIs that allow product teams to query by language, product line, and recency, while keeping latency low and costs predictable. \n\nYou will evaluate embedding models and LLMs with a focus on European languages and noisy industrial terminology. You will build red-team style evaluation sets that include fragmented text, repeated headers, and partial updates to ensure the system remains robust. You will also be responsible for resilience: health checks for Ollama, retry policies, backoff, and metrics around queue depth and token usage. You will add fine-grained logging around chunker decisions (where boundaries were placed, why overlap was chosen) and embedding calls (model, tokens, timing) so operators can troubleshoot without guesswork. \n\nThe role is hands-on with Python, SQLAlchemy, sentence-transformers, and containerized deployments. You will collaborate with security to ensure secrets are managed outside of git, configs are environment-driven, and audit logs capture access to sensitive corpora. You will document the platform with diagrams, quickstart guides, and runbooks for on-call engineers. If you are motivated by building search that users trust, can explain why a result was returned, and stays fast under load, this role will suit you. Expect to work closely with stakeholders in Germany and to ship small, iterative improvements weekly. \n\nWe value engineers who think holistically about data quality, retrieval relevance, and maintainability. You should be comfortable balancing experimentation with a disciplined approach to testing, observability, and rollout safety."
    }
  ]
}
